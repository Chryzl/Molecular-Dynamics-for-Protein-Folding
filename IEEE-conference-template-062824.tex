\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{orcidlink}
\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

\newcommand{\TODO}[1]{\textbf{\textcolor{red}{TODO: #1}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{
    Investigating Protein Folding Dynamics Using Molecular Dynamics Simulations-based Approaches
}

\author{\IEEEauthorblockN{Christian Nix\,\orcidlink{0009-0006-5718-3452}}
\IEEEauthorblockA{\textit{School of Computation, Information and Technology (CIT)} \\
\textit{Technical University of Munich}\\
Munich, Germany \\
christian.nix@tum.de}
}

\maketitle

\begin{abstract}
\TODO{Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi.}
\end{abstract}

\begin{IEEEkeywords}
\TODO{Index words to be added in the end.}
\end{IEEEkeywords}

\section{Introduction}
Proteins are fundamental in driving biological processes and form the basic building blocks that life relies on \cite{eisenberg2000protein}. They are pivotal in almost all cellular processes, thus motivating their study in fields such as medicine, chemistry, or biophysics. Proteins play essential roles in regulation, transportation, signaling, immune responses, and metabolism, among others. \cite[pp.~163ff., pp.~225ff.]{buxbaum2007fundamentals} Chemically, proteins are a polymer of 20 canonical amino acids that have unique side-chains giving them unique chemical properties, such as hydrophobicity, charge, or polarity. Neighboring amino acids are linked via peptide bonds formed between their \textit{amino} and carboxyl (\textit{acid}) groups, thus forming the protein's backbone from which their distinct side-chains extend. \cite{stollar2020uncovering}

Protein structure is commonly described as four hierarchical levels: primary, secondary, tertiary, and quaternary structure. The \textit{primary structure} refers to the linear sequence of amino acids in the protein chain. Given the chemical properties of neighboring amino acids, \textit{secondary structure} elements, such as $\alpha$-helices or $\beta$-sheets, can be formed. The secondary structure is stabilized by hydrogen bonds between backbone atoms. The \textit{tertiary structure} describes the overall 3D arrangement of the protein chain, including the side-chains, i.e., the determination of all atoms in 3D space. The tertiary structure is stabilized by a broad set of interactions, such as van der Waals forces, hydrogen bonds, ionic interactions, and disulfide bridges. Finally, some proteins consist of multiple polypeptide chains (i.e., multiple protein subunits), whose arrangement is described by the \textit{quaternary structure}. \cite[pp.~65ff.]{kessel2018introduction}

Generally speaking, protein function, and thus the protein's biochemical role, is determined by its 3D structure, motivating the study of protein structures and how proteins assume these structures \cite{lee2007predicting}. However, proteins are not static entities, but rather dynamic molecules that can undergo conformational changes to perform their functions. Therefore, understanding protein dynamics is crucial for a comprehensive understanding of protein function. \cite{teilum2009functional} Additionally, misfolded proteins can lead to various diseases, such as Alzheimer's, Parkinson's, or cystic fibrosis, further highlighting the importance of understanding protein folding as a dynamic process \cite{valastyan2014mechanisms}.

\subsection{Protein Folding and its Importance}
So far, we have established that knowing the 3D structure of proteins is important for understanding their function and role in biological processes. However, the process of protein folding itself is highly complex and not yet fully understood \cite{tsuboyama2023mega}. Anfinsen's Nobel prize-winning work demonstrated that the primary structure (i.e., the amino acid sequence) of a protein determines its final 3D structure, implying that all information necessary for folding is self-contained \cite{anfinsen1973principles}. However, if a protein were to randomly sample all possible conformations its primary sequence could take, the time required to fold would be much beyond the biologically relevant timescales (milliseconds to seconds) observed in nature \cite{levinthal1969fold, dobson2003protein}. For example, assuming 10 possible configurations per amino acid residue, a small protein of 100 residues would have $10^{100}$ possible conformations. A rotation around a single bond takes roughly $10^{-12}$ s \cite{lee2025ultrafast}, leading to a total time of $10^{88}$ s or roughly $10^{80}$ years to sample all conformations. This discrepancy is known as Levinthal's paradox \cite{levinthal1969fold}. Since we know that proteins do fold on biologically relevant timescales, there must be some guiding principles that direct the folding process efficiently.

To describe the folding process and its driving forces, we make use of the Gibbs free energy $G = H - TS$, where $H$ is the enthalpy, $T$ the temperature, and $S$ the entropy \cite[pp.~76ff.]{AtkinsDePaula2006}. The Gibbs energy is valid, because folding occurs at constant pressure and temperature \cite{ghelis2012protein, pain2000mechanisms, bryngelson1995funnels}. As becomes apparent from the definition of $G$, folding is driven by enthalpic ($H$) and entropic contributions ($TS$). For example, a main driver of folding is the hydrophobic effect, where non-polar residues cluster together \cite{durell2017hydrophobic}. This benefits the enthalpy via van der Waals forces between the residues, but also increases the entropy by releasing ordered water molecules into the bulk solvent \cite[pp.~280ff.]{KlostermeierRudolph2024}. If we now consider the free energy as a function of the protein's conformation, we arrive at the concept of the energy landscape of protein folding. In Levinthal's paradox, the energy landscape was assumed to be flat, with only the native, folded state being energetically favorable, resulting in random searching. During the 1990s, the concept of the energy landscape as a folding funnel was introduced (Figure \ref{fig:intro:funnel}). \cite{bryngelson1995funnels, onuchic1997theory}

\begin{figure}[b]
    \centering
    \input{figures/folding_funnel.tex}
    \caption{\TODO{Temporary Fig} Schematic representation of a high-dimensional protein folding funnel which is a free energy function $G$ of protein conformation $\mathbf{r}$. The native state corresponds to the global minimum of the free energy. Local minima represent meta-stable folding intermediates which originate from conformational changes. Kinetic conversion between minima requires overcoming energy barriers and can thus be slow.}
    \label{fig:intro:funnel}
\end{figure}

As can be seen in Figure \ref{fig:intro:funnel}, the folding funnel concept correctly captures that conformations along a folding pathway do not have the same energy. The idealized funnel shape arises from the assumption that each newly formed native contact (i.e., a contact that exists in the native structure) lowers the free energy, leading to a monotonic decrease in free energy as folding progresses. \cite[pp.~283ff.]{KlostermeierRudolph2024} However, the energy landscape is much more rugged in reality, with many local extrema. For example, conformations stemming from rotating a side-chain may differ by small differences in energy, thus these conformations are interconverted rapidly. On the other hand, larger conformational changes, such as forming or breaking tertiary structure elements, may lead to greater energy changes and therefore result in local minima that are harder to escape from. \cite{bryngelson1995funnels} These local minima, given sufficiently large energy barriers, can lead to meta-stable states and folding intermediates \cite{dill2008protein}.

The protein folding problem is not limited to predicting the final folded structure, but is also interested in understanding the folding pathways, i.e., the process by which a protein transitions from its unfolded to folded state through these various intermediates and meta-stable states \cite{eaton2000fast}. Ground-breaking methods such as AlphaFold \cite{senior2020improvedAF1, jumper2021highlyAF2, abramson2024accurateAF3} have essentially solved the prediction of the final folded structure, although they do not reveal \emph{how} proteins reach that state \cite{outeiral2022current}. Importantly, proteins do not necessarily fold via a single, well-defined pathway, but can utilize multiple pathways to reach the native state \cite{sridevi2004increasing}. As a result, we must identify not only the final native state, but also the various folding intermediates along the pathways and determine the kinetics of folding, i.e., the rates at which transitions between conformational states occur, especially those that are rate-limiting and therefore control the overall folding speed \cite{gelman2014fast}.

\subsection{Studying Protein Folding Dynamics}
\label{sec:intro:studying_folding}
Until the 1990s, protein folding dynamics were primarily studied experimentally using \textit{stopped-flow} techniques, which allowed a temporal resolution of around 1 ms \cite{gibson19696}. At the same time, atomistic molecular dynamics (MD) simulations were limited to nanosecond timescales. As a result, all (at the time) described protein folding events occurred on timescales inaccessible to MD simulations. \cite{eaton2021modern} However, the development of laser-based temperature-jump (T-jump) techniques enabled the experimental study of protein folding on nanosecond timescales \cite{phillips1995ultrafast}. Nonetheless, T-jump experiments cannot provide information about the underlying conformational rearrangements as they report only global relaxation kinetics \cite{eaton2021modern}. In contrast, experimental methods such as Förster resonance energy transfer (FRET) can yield insights into the distances between specific residues, thus providing some structural information \cite{schuler2008protein}. However, experimental methods are generally limited in the level of detail they can provide about the folding process, motivating the use of MD simulations to allow atomistic insights into folding dynamics \cite{piana2014assessing}.

In molecular dynamics, the motion of atoms is propagated using classical mechanics, by integrating, e.g., Newton’s equations of motion \cite[pp.~358ff.]{santamaria2023molecular}. The forces acting on the atoms are typically obtained from a molecular mechanics force field\footnote{Since MD simulations are fully controlled by the user, the forces acting on the system do not have to correspond strictly to physical reality but can also be altered, e.g., by mixing classical mechanics with quantum mechanical effects \cite{warshel1976theoretical}.}, which provides an approximate potential energy function \cite[pp.~298ff.]{santamaria2023molecular}. As a result, running MD simulations allows us to explore the conformational space according to the Boltzmann distribution (given some assumptions, which we will discuss later), which includes folding events \cite{henin2022enhanced}. However, the exploration of conformational space is still stochastic, requiring us to run simulations for long enough. With increasing computational power \cite{salomon2013routine}, the use of specialized hardware \cite{shaw2008anton}, or highly parallelized simulations \cite{shirts2000screen}, MD simulations can access increasingly long timescales. \cite{hollingsworth2018molecular} Alternatively, enhanced sampling methods such as metadynamics \cite{laio2002escaping} or umbrella sampling \cite{torrie1977nonphysical} can be used to accelerate the exploration of rare conformations by biasing the sampling process, e.g., by adding biasing potential that guides the system over energy barriers \cite{henin2022enhanced}. However, since these methods bias the simulation, the downstream analysis of the resulting trajectories must be adapted accordingly \cite{kamenik2022enhanced}.

To extract meaningful insights from MD simulations, their trajectories must be analyzed appropriately. Some properties, such as the temperature, pressure, or distances, can be computed directly from (averages over) the frames of the trajectory. However, many properties of interest, such as free energies which determine folding pathways and their kinetics, depend not on the frames directly, but rather on the distribution of conformations they represent, i.e., the accessible conformational space. \cite[pp.~167ff.]{frenkel2002understanding} Analyzing the conformational space is non-trivial due to its high dimensionality (e.g., three spatial coordinates per atom). To enable interpretation, we can project the high-dimensional conformational space onto fewer collective variables (CVs), along which we, for example, can compute free energy surfaces (FES). \cite{fiorin2013using} Collective Variables are detailed in Section \TODO{Ref}. For extracting long-timescale kinetics, a probabilistic framework such as Markov State Models (MSMs) can be applied \cite{malmstrom2014application}. These will be discussed in detail in Section \TODO{Ref}. MSMs discretize the conformational space into distinct states, enabling the quantitative prediction of transition rates and folding pathways \cite{chodera2014markov}.

\subsection{Aim and Scope}
This work provides a literature review on computational approaches for investigating protein folding dynamics. We focus specifically on the transfer operator formalism which we summarize for the reader and afterwards leverage as a unifying link between different analysis techniques.

We examine two primary methods for extracting insight from molecular dynamics data. The first is Time-lagged Independent Component Analysis (TICA). This technique identifies the slow collective variables of a system and allows us to visualize free energy surfaces. The second method is Markov State Models (MSMs). These models discretize the simulation data to predict folding pathways and kinetic rates. We explain the intuition behind both methods and how they extract meaningful signals from noisy trajectories. Furthermore, we discuss how the variational principle of the transfer operator connects them. This perspective treats both TICA and MSMs as related tools for finding the slowest physical processes in a system.

% TODO below, change later depending on what I find and actually end up doing
Beyond biological systems, we explore a novel application of these frameworks to machine learning. We draw a parallel between molecular dynamics and neural network training. In this analogy, the network parameters move through a loss landscape much like atoms move through an energy landscape. We investigate whether tools designed for protein folding can provide insights into the training of deep learning models. % TODO -> Specifically, we treat the optimization process as a trajectory. We then test if TICA and MSMs can identify metastable parameter states or convergence pathways in the loss landscape. We demonstrate these concepts on model systems to explore how parameter settings affect the resulting dynamics.

\section{Transfer Operator Formalism}
At the end of the 1990s, the transfer operator formalism was introduced to model the dynamics of molecular systems in the context of MD simulations and thus provided a principled mathematical framework for describing MD simulations \cite{schutte2001transfer}. In this section, we summarize the key concepts of this formalism that are necessary to understand the following sections on collective variables (Section \TODO{Ref}) and Markov State Models (Section \TODO{Ref}). The notation and terminology used here follow those in \cite{prinz2011markov, perez2013identification, noe2017collective}. For more detailed explanations and in-depth derivations, the reader is referred to those references.

The possible conformations of a molecular system, in our case a (solvated) protein, span a high-dimensional state space $\Omega \subseteq \mathbb{R}^{3n}$, where $n$ is the number of atoms in the system. In some cases, the state space may be expanded to represent the full phase space by including momenta (then $\Omega \subseteq \mathbb{R}^{6n}$), but we will focus on the configuration space here. A trajectory generated by an MD simulation can be seen as a time series of points in this state space $\mathbf{x}(t) = \mathbf{x} \in \Omega$, i.e., the conformation of the system at time $t$. The dynamical process $\mathbf{x}(t)$ can be either time continuous or discrete. Since we are concerned with MD simulations, we will consider discrete time points $t = k \delta$, where $k \in \mathbb{N}_0$ and $\delta$ is the integration time step between frames. However, the transfer operator formalism will increasingly move away from individual trajectories or conformations and instead focus on probability distributions over the state space $\Omega$ at time $t$, denoted as $p_t(\mathbf{x}): \Omega \rightarrow \mathbb{R}_{0+}$. These probability distributions describe how likely the system is to occupy different regions of the state space at a given time $t$ and thus provide a statistical description of the ensemble that the MD simulation sampled from. We must impose some assumptions on the dynamical process $\mathbf{x}(t)$ to make the following formalism mathematically founded.

First, we assume that the process is \textit{Markovian} in $\Omega$. This means that the future state of the system $\mathbf{x}(t + \Delta t)$ depends only on its current state $\mathbf{x}(t)$. We can therefore define the transition probability density as:
Based on the discretization, we can define the transition probability matrix $\mathbf{T}(\tau) \in \mathbb{R}^{n \times n}$, 
\begin{equation}
    p(\mathbf{x}, \mathbf{y}; \tau) d\mathbf{y} = \mathbb{P}\left[\mathbf{x}(t + \tau) = \mathbf{y} + d\mathbf{y} | \mathbf{x}(t) = \mathbf{x}\right] \label{eq:intro:transition_prob}
\end{equation}
$p(\mathbf{x}, \mathbf{y}; \tau)$ describes the probability density of transitioning from state $\mathbf{x}$ at time $t$ to state $\mathbf{y}$ at time $t + \tau$ (note that we require $d\mathbf{y}$ since the state space is continuous and we must integrate to get probabilities). As before, we will consider discrete time points, i.e., $\tau = m \delta$ for $m \in \mathbb{N}_0$. Similarly, we can define the transition probability from a state $\mathbf{x}$ at time $t$ to a set of states $A \subset \Omega$ at time $t + \tau$ as $p(\mathbf{x}, A; \tau) = \mathbb{P}\left[\mathbf{x}(t + \tau) \in A | \mathbf{x}(t) = \mathbf{x}\right]$.

Second, we assume that the process is (sufficiently) \textit{ergodic}, meaning that all states in $\Omega$ can be reached from any other state, such that a unique stationary distribution $\mu(\mathbf{x})$ exists. The stationary distribution can be seen as the fraction of time the system spends in different regions of the state space when the simulation time approaches infinity. As mentioned in Section \ref{sec:intro:studying_folding}, the stationary distribution for molecular dynamics simulations at constant temperature is given by the Boltzmann distribution:
Based on the discretization, we can define the transition probability matrix $\mathbf{T}(\tau) \in \mathbb{R}^{n \times n}$, 
\begin{equation}
    \mu(\mathbf{x}) = Z^{-1} e^{-\beta H(\mathbf{x})} \label{eq:intro:boltzmann}
\end{equation}
where $Z$ is the partition function (normalization constant), $\beta = (k_B T)^{-1}$ is the inverse temperature with Boltzmann constant $k_B$ and temperature $T$, and $H(\mathbf{x})$ is the Hamiltonian of the system at conformation $\mathbf{x}$ (total energy).

Lastly, we assume that the process is \textit{reversible}. Thus, the transition probability satisfies the condition of detailed balance, i.e., when at equilibrium (the stationary distribution $\mu(\mathbf{x})$), the transition rate from state $\mathbf{x}$ to $\mathbf{y}$ is equal to the reverse direction: $\mu(\mathbf{x}) p(\mathbf{x}, \mathbf{y}; \tau) = \mu(\mathbf{y}) p(\mathbf{y}, \mathbf{x}; \tau)$. The condition of detailed balance is not necessary for, e.g., Markov State Models (Section \TODO{Ref}), but it simplifies the mathematical treatment so that we will assume it here. Generally, it is assumed that equilibrium MD simulations fulfill this condition \cite{prinz2011markov}.

Given these assumptions, we now focus on the change of the probability distribution $p_t(\mathbf{x})$ (different from $\mu(\mathbf{x})$) as the MD simulation progresses. If the system is at distribution $p_t(\mathbf{x})$ at time $t$, the distribution after lag-time $\tau$ will have changed (slowly approaching $\mu(\mathbf{x})$) since the system has evolved according to the transition probabilities $p(\mathbf{x}, \mathbf{y}; \tau)$ defined above. Mathematically, we can describe this change using the \textit{propagator operator} $\mathcal{P}(\tau)$, which advances the distribution $p_t(\mathbf{x})$ by lag-time $\tau$ to $p_{t + \tau}(\mathbf{y})$:
Based on the discretization, we can define the transition probability matrix $\mathbf{T}(\tau) \in \mathbb{R}^{n \times n}$, 
\begin{equation}
    p_{t + \tau}(\mathbf{y}) = \mathcal{P}(\tau) p_t(\mathbf{y}) = \int_{\mathbf{x} \in \Omega} p(\mathbf{x}, \mathbf{y}; \tau) p_t(\mathbf{x}) d\mathbf{x} \label{eq:intro:propagator}
\end{equation}
If we now consider not the probability distribution $p_t(\mathbf{x})$ directly, but rather its relative distribution with respect to the stationary distribution $\mu(\mathbf{x})$, we can define the equivalent \textit{transfer operator} $\mathcal{T}(\tau)$ as:
Based on the discretization, we can define the transition probability matrix $\mathbf{T}(\tau) \in \mathbb{R}^{n \times n}$, 
\begin{align}
    u_{t + \tau}(\mathbf{y}) = \mathcal{T}(\tau) u_t(\mathbf{y}) &= \frac{1}{\mu(\mathbf{y})} \int_{\mathbf{x} \in \Omega} p(\mathbf{x}, \mathbf{y}; \tau) \mu(\mathbf{x}) u_t(\mathbf{x}) d\mathbf{x} \\
    p_t(\mathbf{x}) &= \mu(\mathbf{x}) u_t(\mathbf{x}) \label{eq:intro:prop_to_transfer}
\end{align}
Because both operators $\mathcal{P}(\tau)$ and $\mathcal{T}(\tau)$ fulfill the Chapman-Kolmogorov equation, they can be applied arbitrarily often to propagate the distribution over multiple lag-times $\tau$.

The \emph{key insight} of the transfer operator formalism lies in the recognition that the eigenvalues and eigenfunctions of the propagation and transfer operators hold interpretable meanings. This is most easily seen by considering the action of the propagator operator on the stationary distribution $\mu(\mathbf{x})$. Since the system is already at equilibrium, applying the propagator does not change the distribution:
\begin{equation}
    \mathcal{P}(\tau) \mu(\mathbf{x}) = \mu(\mathbf{x}) = \phi_1(\mathbf{x})
\end{equation}
Thus, the stationary distribution $\mu(\mathbf{x})$ is an eigenfunction $\phi_1(\mathbf{x})$ of the propagator operator with eigenvalue $\lambda_1 = 1$. By Equation \ref{eq:intro:prop_to_transfer}, we can see that the corresponding eigenfunction of the transfer operator is simply the constant function $\psi_1(\mathbf{x}) = \mathbf{1}$. More generally, we can define the eigenvalue problems for both operators as:
\begin{align}
    \mathcal{P}(\tau) \phi_i(\mathbf{x}) &= \lambda_i \phi_i(\mathbf{x}) \label{eq:intro:evp_prop} \\
    \mathcal{T}(\tau) \psi_i(\mathbf{x}) &= \lambda_i \psi_i(\mathbf{x}) \label{eq:intro:evp_transfer}
\end{align}
where we can relate the eigenfunctions of both operators via $\phi_i(\mathbf{x}) = \mu(\mathbf{x}) \psi_i(\mathbf{x})$, following from Equation \ref{eq:intro:prop_to_transfer}. Importantly, the eigenvalues $\lambda_i$ are the same for both operators. If we assume reversibility, the eigenvalues are real and can be bounded by $-1 < \lambda_i \leq 1$. Following convention, we can order the eigenvalues such that $\lambda_1 = 1 > \lambda_2 \geq \lambda_3 \geq ...$. However, we only consider the biggest $m$ eigenvalues and eigenfunctions termed dominant. From the Chapman-Kolmogorov equation, it follows that the eigenvalues must follow a relation to a physically interpretable timescale $t_i$ in the form of:
\begin{equation}
    \lambda_i = e^{-\tau t_i^{-1}} \quad \Leftrightarrow \quad t_i = -\frac{\tau}{\ln \lambda_i}
\end{equation}
These timescales $t_i$ are also referred to as the $i$th implied timescale.

Combining the above (assuming reversibility) allows us to decompose the action of the transfer operator on an arbitrary distribution $u(\mathbf{x})$ into its eigenvalues and eigenfunctions:
\begin{align}
    u_{t + k\tau}(\mathbf{x}) &= \mathcal{T}(\tau)^k u_t(\mathbf{x}) = \sum_{i=1}^{\infty} \lambda_i^k \langle u_t, \phi_i \rangle \psi_i(\mathbf{x}) \\
    &= \sum_{i=1}^{m} \lambda_i^k \langle u_t, \phi_i \rangle \psi_i(\mathbf{x}) \left[ + \mathcal{T}(\tau)^k_{\text{fast}} u_t(\mathbf{x}) \right] \label{eq:intro:transfer_decomp}
\end{align}
where $\langle u_t, \phi_i \rangle = \int_{\mathbf{x} \in \Omega} u_t(\mathbf{x}) \phi_i(\mathbf{x}) d\mathbf{x}$ is the scalar product in Dirac notation.

Equation \ref{eq:intro:transfer_decomp} allows us to interpret the dynamics in a physically meaningful way. We split the decomposition into the $m$ dominant and the remaining fast processes since we are primarily interested in the slow processes of the system, which include folding or larger conformational changes. Thus, the slow dynamics can be viewed as a linear combination of the $m$ dominant eigenfunctions. With increasing simulation time (i.e., increasing $k$), the contributions of the eigenfunctions scales exponentially with their eigenvalues. For all eigenfunctions, except the stationary, this contribution will eventually decay to zero as $k \rightarrow \infty$, returning the distribution to the stationary distribution. Larger timescales $t_i$ correspond to eigenvalues $\lambda_i$ closer to 1, thus these processes decay more slowly and represent, e.g., a distribution of meta-stable states which must overcome large energy barriers to relax to equilibrium. Faster processes have smaller eigenvalues and thus decay quickly, allowing us to approximate the long-timescale dynamics using only the $m$ dominant eigenvalues and eigenfunctions.

\subsection{Variational Approach for Approximating the Eigenvalue Problem}

As seen above, the eigenvalues and eigenfunctions of the transfer operator provide a physically interpretable description of the long-timescale dynamics of molecular systems. Therefore, we are interested in finding the solutions to the eigenvalue problem defined in Equations \ref{eq:intro:evp_prop} and \ref{eq:intro:evp_transfer}. However, we cannot solve these equations analytically for realistic molecular systems. Instead, we must resort to numerical approximations from MD data. \cite{noe2013variational} For this, we will look at two approaches in the following sections: Time-lagged Independent Component Analysis (TICA) in Section \TODO{Ref} and Markov State Models (MSMs) in Section \TODO{Ref}. How \emph{both} methods approximate the eigenvalue problem can be understood within the framework of the \textit{variational principle}, where each method is a special case of this principle \cite{noe2013variational,perez2013identification}. Here, we will only summarize the key concepts of the variational principle. For in-depth discussions, derivations, and proofs, we refer the reader to \cite{noe2013variational, perez2013identification, noe2017collective}.

In the variational approach, we approximate the eigenfunctions $\psi_i(\mathbf{x})$ of the transfer operator $\mathcal{T}(\tau)$ as linear combinations of a set of $n$ basis functions $\chi_k(\mathbf{x})$:
\begin{equation}
    \hat{\psi}_i(\mathbf{x}) = \sum_{k=1}^{n} b_{ik} \chi_k(\mathbf{x}) \label{eq:intro:var_approx}
\end{equation}
where $\hat{\psi}_i(\mathbf{x})$ is the approximation of $\psi_i(\mathbf{x})$ and $b_{ik}$ are the expansion coefficients which we can collect into a vector $\mathbf{b}_i \in \mathbb{R}^n$. The approximation of the eigenvalue problem is thus reduced to finding optimal coefficients $\mathbf{b}_i$ for each eigenfunction. Based on a generalized Ritz method \cite{ritz1909neue}, we can show that the optimal coefficients $\mathbf{b}_i$ for any basis set $\{\chi_k(\mathbf{x})\}_{k=1}^n$ can be found by solving the generalized eigenvalue problem:
\begin{equation}
    \mathbf{C}(\tau) \mathbf{b}_i = \mathbf{C}(0) \hat{\lambda}_i \mathbf{b}_i \label{eq:intro:var_evp}
\end{equation}
where $\hat{\lambda}_i$ is the approximation of $i$th eigenvalue. The matrices $\mathbf{C}(\tau)$ and $\mathbf{C}(0)$ are the covariance matrices of the basis functions at lag-time $\tau$ and 0, respectively:
\begin{align}
    c_{ij}^{\chi}(\tau) &= \langle \chi_i(\mathbf{x}(t)) \cdot \chi_j(\mathbf{x}(t + \tau)) \rangle_{t} \label{eq:intro:cov_mat_tau} \\
    c_{ij}^{\chi}(0) &= \langle \chi_i(\mathbf{x}(t)) \cdot \chi_j(\mathbf{x}(t)) \rangle_{t} \label{eq:intro:cov_mat_nill}
\end{align}
where $\langle \dots \rangle_{t}$ denotes the time average over the MD trajectory, not the scalar product. By solving Equation \ref{eq:intro:var_evp}, we can obtain the optimal coefficients $\mathbf{b}_i$ and the approximated eigenvalues $\hat{\lambda}_i$. We can therefore approximate the eigenfunctions of the transfer operator via Equation \ref{eq:intro:var_approx} as linear combinations of the basis functions. These basis functions $\chi_k(\mathbf{x}): \Omega \rightarrow \mathbb{R}$ can generally be chosen freely and can be non-linear functions of the state space, thus allowing flexibility despite a linear combination.

\section{Collective Variables and Free Energy Surfaces}
Molecular simulation data is inherently high-dimensional, as it describes the positions of all atoms in a system over time. Therefore, directly interpreting this data to extract meaningful insights is challenging. \cite{glielmo2021unsupervised} Collective Variables (CVs) provide a solution to this problem by reducing the dimensionality of the data. Specifically, CVs are functions of conformational space that map to scalar values: $\xi: \Omega \rightarrow \mathbb{R}$. \cite{henin2022enhanced} In Figure \ref{fig:intro:funnel}, we have already implicitly used a CV by plotting the free energy as a function of protein conformation. To capture meaningful aspects of the system's dynamics, CVs should ideally separate different meta-stable states and capture the slow dynamics of the system \cite{bhakat2022collective}. For example, in protein folding, an ideal CV would range from 0 (unfolded) to 1 (folded), effectively capturing the folding process and clearly separating meta-stable states along this CV. Many approaches to define CVs have been proposed, ranging from those based on chemo-physical intuition (e.g., radius of gyration \cite{ivankov2009coupling}, dihedral angles \cite{jerath2016mapping}, distances between residues \cite{taddese2020deciphering}) to data-driven methods (e.g., Principal Component Analysis (PCA) \cite{maisuradze2009principal}, Time-lagged Independent Component Analysis (TICA) \cite{schwantes2013improvements}, autoencoders \cite{belkacemi2023autoencoders}) \cite{bhakat2022collective}. CVs have been used in molecular simulations for decades, with early applications dating back to the 1970s \cite{chandler1978statistical}.

Generally, we must differentiate between CVs used for analysis, i.e., constructed after the simulation, and CVs used for enhanced sampling, i.e., defined before the simulation \cite{bonati2021deep}. If we look at CVs for analysis, we can leverage our simulation data itself to find CVs that capture the most important dynamics of the specific system we are interested in. Thus, we have a broad range of data-driven methods at our disposal. \cite{glielmo2021unsupervised} In contrast, we can also leverage CVs for enhanced sampling by biasing the simulation along them to accelerate the exploration of rare events (see Section \ref{sec:intro:studying_folding}). However, in this case, we must decide on the CVs before running the simulation, as they cannot be estimated from the simulation data itself \footnote{There are methods to refine CVs on-the-fly during the simulation, however, they are limited to the data up until any given point \cite{belkacemi2021chasing}.} \cite{stracke2024use}. In this report, we will focus on CVs for analysis. Nonetheless, extensive research efforts are also dedicated to finding CVs for enhanced sampling, with an increasing focus on machine learning approaches \cite{bhakat2022collective, glielmo2021unsupervised, henin2022enhanced}.

After defining suitable CVs, we can, e.g., compute the free energy as a function of these CVs, resulting in an approximated free energy surface (FES). The FES provides a low-dimensional representation of the energy landscape of the system, allowing us to visualize and the dynamics more easily, thus giving us the ability to form intuition about the folding process. \cite{henin2022enhanced} How to accurately compute free energies from MD data is a complex topic beyond the scope of this report \cite[pp.~18ff.]{chipot2007free}. However, the key idea can be understood by rewriting the Boltzmann distribution (see Equation \ref{eq:intro:boltzmann}) and solving for the energy:
\begin{equation}
    \mu(\mathbf{x}) = Z^{-1} e^{-\beta H(\mathbf{x})} \; \Rightarrow \; - \beta^{-1} \ln \mu(\mathbf{x}) = H(\mathbf{x}) + C
\end{equation}
where $C = \beta^{-1} \ln Z$ is a constant. Thus, we can approximate the free energy if we know the probability distribution over our state space projected onto the CVs. An example FES along TICA CVs for protein folding can be found in Figure \TODO{Ref}. From the FES, we can clearly visualize the \TODO{States or something}. Importantly, we must be cautious when interpreting FES, as they are only low-dimensional projections of a high-dimensional landscape, necessarily leading to a loss of some information \cite{ahalawat2018assessment}. This can result in the so-called hysteresis problem in MD analysis, where important states or transitions are hidden \cite{lichtinger2023tackling}. More advanced tools like Markov State Models (MSMs) discussed in Section \TODO{Ref} can help interpret the dynamics more accurately. These models also often rely on CVs for discretization, where the MD data is clustered along the CVs to identify meta-stable states. \cite{konovalov2021markov}

\subsection{Time-lagged Independent Component Analysis}
One of the most common data-driven methods to define CVs is Time-lagged Independent Component Analysis (TICA) \cite{noe2017collective}. TICA was originally introduced as a signal processing technique \cite{ans1985adaptive}, however, it has found widespread application in the analysis of molecular dynamics simulations going back to 2011 \cite{naritomi2011slow}. Similar to Principal Component Analysis (PCA) \cite{pearson1901liii}, TICA seeks to find linear combinations of input features that capture important aspects of the data. However, while PCA identifies directions of maximum variance \cite{abdi2010principal}, TICA focuses on finding uncorrelated components that are maximally autocorrelated over a specified lag-time $\tau$ \cite[pp.~341ff.]{hyvarinen2001independent}. The application of TICA over PCA in MD analysis was initially motivated by the fact that principle components are not Independent of each other, undermining the validity of linearly decomposing the dynamics into these components \cite{naritomi2011slow}. Later, it was shown that TICA can be understood as a special case of the variational approach for linearly approximating the eigenvalue problem of the transfer operator (see Section \TODO{Ref}) \cite{noe2013variational}. Thus, TICA is particularly well-suited for identifying the slow collective variables of molecular systems with a mathematical foundation rooted in the transfer operator formalism \cite{perez2013identification}.

Following \cite{perez2013identification}, we can show that TICA arises as a special case of the variational approach. As mentioned before, we aim to reduce the dimensionality of our spacial data $\mathbf{x}(t)$ (vector of atomic coordinates) by finding uncorrelated (Equation \ref{eq:intro:tica_cov0}) but maximally autocorrelated (at lag time $\tau$, Equation \ref{eq:intro:tica_covtau}) components, defined as:
\begin{align}
    c_{ij}^x (0) &= \langle x_i(t) \cdot x_j(t) \rangle_{t} \quad\;\;\, \stackrel{!}{=} \delta_{ij} \label{eq:intro:tica_cov0} \\
    c_{ii}^x (\tau) &= \langle x_i(t) \cdot x_i(t + \tau) \rangle_{t} \stackrel{!}{=} \text{maximal} \label{eq:intro:tica_covtau}
\end{align}
where $x_i(t)$ is the $i$th atomic coordinate Component at time $t$ and $\delta_{ij}$ is the Kronecker delta. To achieve uncorrelated components (Equation \ref{eq:intro:tica_cov0}), we aim to diagonalize the covariance matrix $\mathbf{C}^x(0)$ via a linear transformation $\mathbf{W} = \left[\mathbf{w_1}, \dots, \mathbf{w_d}\right]$. Simultaneously, we want to maximize the autocorrelation at lag-time $\tau$ (Equation \ref{eq:intro:tica_covtau}) with respect to this transformation. P\'erez-Hern\'andez et al. \cite{perez2013identification} show that both objectives can be achieved by solving the generalized eigenvalue problem:
\begin{equation}
    \mathbf{C}^x(\tau) \mathbf{w}_i = \mathbf{C}^x(0) \hat{\lambda}_i \mathbf{w}_i \label{eq:intro:tica_evp}
\end{equation}
which is equivalent to the variational eigenvalue problem defined in Equation \ref{eq:intro:var_evp} when choosing the basis functions as the input coordinates themselves, i.e., $\chi_k(\mathbf{x}) = x_k$.

However, solving Equation \ref{eq:intro:tica_evp} is not trivial because some coordinates are most likely highly correlated. Therefore, the covariance matrix $\mathbf{C}^x(0)$ may be ill-conditioned, making it difficult to solve the eigenvalue problem directly. \cite{perez2013identification} The problem is typically solved in two steps via the AMUSE algorithm \cite{Tong1990AMUSE}, which first whitens the data by applying PCA and then solves a standard eigenvalue problem on the whitened data \cite{perez2013identification}. Thus, TICA gives us a powerful tool to approximate the slow eigenfunctions of the transfer operator laying the foundation for further analysis techniques such as Markov State Models (Section \TODO{Ref}).

% TODO Commute maps? From the Noe 2017 review

\section{Markov State Models and Protein Folding Kinetics}

% --- Subsection: Motivation and the Need for Discretization ---
% Goal: Explain why we move from TICA/FES to MSMs.
% - Recap: We have TICA for slow coordinates, but FES is just a projection (loss of info).
% - Problem: We want quantitative kinetics (rates, timescales) which are hard to get from FES alone due to projection errors.
% - Solution: Discretize the state space into microstates to model transitions explicitly.
% - [Existing] Despite comp advancements --> Time scale is still limited, especially since we want a statistical picture (many rare transitions) to get kinetics (Chodera 2014)
% - [Existing] MSMs allow to tackle that problem
% - [Existing] MSMs can (bacause of markovianity) stitch together short simulations --> longer timescales (Chodera 2014)
% - [Existing] CVs are nice and all, but we are always losing information + we can only do so much with them --> MSMs allow more (Prinz 2011)
While TICA linearly approximates the slowest eigenfunctions of the transfer operator, it is limited in its ability to provide exact quantitative kinetic insights into molecular systems. This is in part due to the projection of the full dynamics onto a low-dimensional space, as well as that linear approximations of the eigenfunctions may fall short in capturing the true eigenfunctions, especially in complex systems with non-linear dynamics. To overcome these and more limitations, we can employ Markov State Models (MSMs), which discretize the state space and explicitly model transitions between these discrete states. MSMs additionally give us the ability to stitch together many short simulations to approximate long-timescale dynamics (such as folding kinetics) without having to run prohibitively long simulations. Thus, MSMs are also an alternative to enhanced sampling methods briefly discussed in Section \ref{sec:intro:studying_folding}.

% --- Subsection: Physical Insights and Kinetics ---
% Goal: What biological/physical answers do we get?
% - What do we get? Eigenvalues (rates), Eigenvectors (processes), Stationary dist (thermodynamics).
% - [Existing] We get kinetics from MSMs via: rate theory or or based on the MD data directly (Prinz 2011)
% - [Existing] Also MSMs do not need to define short order parameters beforehand, different to many enhanced sampling methods (Chodera 2014)
% - [Existing] MSMs can be basis for Transition Path Theory (TPT): https://doi.org/10.1137/070699500
% - [Existing] We can get committors and fluxes between states
As we will see in the following sections, the transition matrix of an MSM directly approximates the transfer operator. Therefore, by solving the eigenvalue problem of the transition matrix, we can obtain approximations of the eigenvalues and eigenfunctions of the transfer operator. This allows us to extract important kinetic information about the molecular system. Specifically, the eigenvalues of the transition matrix correspond to the relaxation rates of different processes in the system, allowing us to compute implied timescales for transitions between meta-stable states. The eigenfunctions provide insights into the nature of these processes, revealing which states are involved in slow transitions. Additionally, the stationary distribution of the MSM gives us thermodynamic information about the relative stability of different states. Beyond these insights, MSMs can also serve as a basis for Transition Path Theory (TPT), which allows us to compute committor probabilities (probability of reaching a target state before returning to the initial state) and fluxes (rates of transitions) between states, providing a deeper understanding of the pathways and mechanisms underlying molecular processes.

\subsection{Markov State Model Construction}
% --- Subsection: Construction and Validation ---
% Goal: Briefly explain the practical steps to build an MSM.
% - Step 1: Discretization/Clustering. Usually done in TICA space to capture slow dynamics.
% - [Existing] MSMs discretize the state space into states (by clustering the data) --> Need to balance between having many states (low discretization error) and few states (good statistics per state) (Chodera 2014)
% - [Existing] We get our states by running e.g. k-means clustering in TICA space (Perez 2013)
% - [Existing] Then assign each frame to a state --> get discrete trajectory (Perez 2013)
% - Step 2: Transition Matrix Estimation. Counting jumps between states at lag time tau.
% - [Existing] MSMs model how likely a transition between states is over a lag time tau --> Transition matrix, akin to propagator (Chodera 2014)
% - [Existing] From transition matrix --> get stationary distribution
% - [Existing] And transition matrix can be gotten from "cheap" short simulations --> Even then we can get the stationary distribution!!!
% - Step 3: Validation. Chapman-Kolmogorov test and Implied Timescales.
% - [Existing] The main kinetic insight we get from MSMs is the implied timescales
% - [Existing] Also, we can do statistical error estimations (Prinz 2011)
To construct an MSM from MD simulation data, we must first discretize the continuous state space into a finite set of discrete states, such as folding intermediates or meta-stable conformations. Since the discretization of the state space will inevitably introduce some error, all our insights from the MSM will be approximations subject to this \textit{discretization error}. A common and intuitive approach to discretize the state space is by first reducing the dimensionality of the MD data using TICA (see Section \TODO{Ref}) and then cluster the data in the space spanned by the linear approximation of the slowest eigenfunctions using algorithms such as k-means clustering. As a result, we can partition the state space into $n$ discrete microstates $S_i$. Each frame of the MD trajectory can then be assigned to one of these states by finding the closest cluster center compared to the conformation $\mathbf{x}$ in the frame, resulting in a discrete trajectory over the states and the class functions:
\begin{equation}
    \chi_i(\mathbf{x}) =
    \begin{cases}
        1, & \mathbf{x} \in S_i \\
        0, & \mathbf{x} \notin S_i
    \end{cases}
    \quad \forall i = 1, \dots, n \label{eq:msm:class_func}
\end{equation}

Based on the discretization, we can define the row-stochastic transition probability matrix $\mathbf{T}(\tau) \in \mathbb{R}^{n \times n}$, where $\mathbf{T}_{ij}(\tau)$ describes the probability of transitioning from state $S_i$ at time $t$ to state $S_j$ at time $t + \tau$:
\begin{equation}
    \mathbf{T}_{ij}(\tau) = \mathbb{P}\left[\mathbf{x}(t + \tau) \in S_j | \mathbf{x}(t) \in S_i\right] \label{eq:msm:trans_ij}
\end{equation}
Given a discrete state distribution vector $\mathbf{p}(t) \in \mathbb{R}^n$ at time $t$, where $\mathbf{p}_{i}(t)$ is the probability of being in state $S_i$ at time $t$, we can propagate this distribution by lag-time $\tau$ using the transition matrix:
\begin{align}
    \mathbf{p}_{j}(t + \tau) &= \sum_{i=1}^{n} \mathbf{p}_{i}(t) \mathbf{T}_{ij}(\tau) \\
    \mathbf{p}^T(t + \tau) &= \mathbf{p}^T(t) \mathbf{T}(\tau) \label{eq:msm:trans_mat_act}
\end{align}

If we compare Equation \ref{eq:msm:trans_ij} to the definition of the general transition probability density in Equation \ref{eq:intro:transition_prob}, as well as compare the action of the transition matrix in Equation \ref{eq:msm:trans_mat_act} to the action of the propagator operator in Equation \ref{eq:intro:propagator}, we can see that the transition matrix $\mathbf{T}(\tau)$ is a discrete approximation of the propagator operator $\mathcal{P}(\tau)$. Therefore, by solving the eigenvalue problem of the transition matrix, we can obtain approximations of the eigenvalues and eigenfunctions of the transfer operator, which includes the stationary distribution:
\begin{equation}
    \mathbf{\pi}^T \mathbf{T}(\tau) = 1 \cdot \mathbf{\pi}^T \label{eq:msm:evp}
\end{equation}
where $\mathbf{\pi} \in \mathbb{R}^n$ is the stationary distribution over the discrete states, compared to $\mu(\mathbf{x})$ in the continuous case. As a result, we can recognize that MSMs are indeed a special case of the variational approach for approximating the eigenvalue problem of the transfer operator (Section \TODO{Ref}) when choosing the class indicator functions defined in Equation \ref{eq:msm:class_func} as basis functions.

% How to get them from data
So far, we have only seen the general definition of the transition matrix (Equation \ref{eq:msm:trans_ij}). Practically, we must estimate the transition matrix from the discrete trajectory obtained from the MD data $\mathbf{x}(t)$ where $t$ is discretized by the simulation time-step $\delta t$. To do so, we first define the observed count matrix $\mathbf{N}^\text{obs}(l \cdot \delta t)$:
\begin{equation}
    \mathbf{N}^\text{obs}_{ij}(l \cdot \delta t) = \sum_{t=0}^{M - l} \chi_{i}(\mathbf{x}(t)) \cdot \chi_{j}(\mathbf{x}(t + l \cdot \delta t)) \label{eq:msm:count_ij}
\end{equation}
which counts the number of observed transitions from state $S_i$ to state $S_j$ over a lag-time of $l \cdot \delta t$ (see Equation \ref{eq:msm:class_func}). Here, $M$ is the total number of frames in the MD trajectory. Similarly, we define the total count $\mathbf{N}^\text{obs}_i(l \cdot \delta t)$ over a state $S_i$ as $\mathbf{N}^\text{obs}_{i}(l \cdot \delta t) = \sum_{j=1}^{n} \mathbf{N}^\text{obs}_{ij}(l \cdot \delta t) \label{eq:msm:total_count_i}$, which counts the total number of observed transitions originating from state $S_i$. Prinz et al. \cite{prinz2011markov} show that the intuitive definition of the transition matrix elements as the ratio of observed transitions from state $S_i$ to state $S_j$ over the total observed transitions from state $S_i$ is a maximum likelihood estimator that is unbiased when approaching infinite simulation length $M \rightarrow \infty$:
\begin{equation}
    \hat{\mathbf{T}}_{ij}(l \cdot \delta t) = \frac{\mathbf{N}^\text{obs}_{ij}(l \cdot \delta t)}{\mathbf{N}^\text{obs}_{i}(l \cdot \delta t)} \label{eq:msm:trans_est}
\end{equation}
However, Prinz et al. \cite{prinz2011markov} also find that $\hat{\mathbf{T}}$ does not necessarily fulfill detailed balance, even if the underlying dynamics are reversible. If this property is desired, there are various methods ensure reversibility in the estimated transition matrix \cite{prinz2011markov}.

% How to validate an MSM (Chapman-Kolmogorov test + Implied Timescales)




% --- Subsection: MSMs as a Variational Approximation ---
% Goal: Connect MSMs back to the Transfer Operator theory established earlier.
% - Link back to Transfer Operator: MSMs are not just heuristic; they fit the variational framework.
% - Basis Functions: Instead of linear functions (TICA), MSMs use characteristic (indicator) functions on the discretized states.
% - [Existing] And again, we can link MSMs to the transfer operator formalism via the variational principle using class indicator functions as basis functions (Chodera 2014)
% - [Existing] ==> Thus, like in QM, defining the basis set can improve the approximation
% - [Existing] MSMs class function can also be a gaussian --> fuzzy clustering (Noe 2017)

% Potential Projects
% file:///Users/christianmacbook/Downloads/978-3-540-38448-9.pdf --> Chapter 8 talks about some math for MD that needs to be assumed (pdf page 293)
% ------------
% - A general concern with TICA and the variational approach is computational efficiency for large systems and long trajectories. The computational effort scales with N^2 for N input coordinates, which becomes intractable if huge basis sets are used, such as the set of distances between all residues or atoms of a protein. The recently proposed hierarchical TICA method can obtain a coarse-grained yet accurate solution of the full TICA problem more efficiently [60], but there is still room for improvements in this area. (from Noe review)
%==> In https://doi.org/10.1063/1.4811489: It is shown that the scalar product of the eigenfunctions (which are the linear combinations of input order parameters) approximates the relaxation timescale from below. If we simply maximize this quantity via gradient ascent, we can obtain the linear coefficients for the eigenfunctions without computing the covariance matrices (in turn avoiding the N^2 scaling, but we need an iterative method). But from what I am just reading, it may be that the approximation is only valid for the slowest timescale (first non-trivial eigenfunction)
%--------------
% - Or what if we apply the formalism found here to the problem of training NNs. From ChatGPT: Use stochastic gradient MCMC (SGLD / SGHMC) to sample: If you want a stationary distribution 𝑝 ( 𝜃 ) ∝ 𝑒 − 𝛽 𝐿 ( 𝜃 ) p(θ)∝e −βL(θ) over parameters, use SGLD: these add calibrated Gaussian noise to gradient updates and have a known stationary distribution (under assumptions). They are exactly the analogue of Langevin / Hamiltonian dynamics in parameter space. Classic refs: Welling & Teh (SGLD) and SGHMC work. (maybe this is (koopman training) related: https://arxiv.org/pdf/2006.02361) IMPORTANT for applying TICA: "We also assume that the dynamics are statistically reversible, i.e., that the molecular system is simulated in thermal equilibrium." from https://doi.org/10.1063/1.4811489 --> Check if this is given

% Free Energy plots http://www.emma-project.org/latest/tutorials/notebooks/02-dimension-reduction-and-discretization.html

% Convergence of 𝑡 𝑖 ( 𝜏 ) t i ​ (τ) as you increase 𝜏 τ is a standard check that your MSM is Markovian at lag 𝜏 τ

\section*{Acknowledgment}

\TODO{Acknowledgments to be added in the end.}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
